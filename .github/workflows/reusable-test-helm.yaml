name: reusable-test-helm

on:
  workflow_call:
    inputs:
      instance-type:
        description: EC2 instance type
        required: false
        default: m7i.metal-24xl
        type: string
      collector-image-artifact:
        description: Name of collector image artifact to use
        required: false
        default: collector-image
        type: string
      nri-init-image-artifact:
        description: Name of nri-init image artifact to use
        required: false
        default: nri-init-image
        type: string
      collector-image-repo:
        description: Collector image repository (e.g., local/collector)
        required: false
        default: local/collector
        type: string
      collector-image-tag:
        description: Collector image tag
        required: true
        type: string
      nri-init-image-repo:
        description: NRI init image repository (e.g., local/nri-init)
        required: false
        default: local/nri-init
        type: string
      nri-init-tag:
        description: NRI init image tag
        required: true
        type: string
      s3-bucket:
        description: S3 bucket to validate outputs
        required: false
        default: unvariance-collector-test-irsa
        type: string
      region:
        description: AWS region
        required: false
        type: string
    secrets:
      AWS_ROLE_ARN:
        required: false
      REPO_ADMIN_TOKEN:
        required: false
      AWS_REGION:
        required: false

jobs:
  setup-runner:
    name: Start EC2 runner
    runs-on: ubuntu-latest
    outputs:
      runner-label: ${{ steps.start-runner.outputs.runner-label }}
      ec2-instance-id: ${{ steps.start-runner.outputs.ec2-instance-id }}
      region: ${{ steps.start-runner.outputs.region }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Start AWS Runner
        id: start-runner
        uses: ./.github/actions/aws-runner
        with:
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          iam-role-name: github-actions-runner
          instance-type: ${{ inputs.instance-type }}
          image-type: ubuntu-24.04

  k3s-deployment:
    needs: [setup-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 10
    env:
      HOME: /root
    steps:
      - name: Checkout .github folder
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          sparse-checkout: |
            .github
          sparse-checkout-cone: true
      - name: Create HOME directory
        run: |
          mkdir -p $HOME

      - name: Setup k3s cluster
        uses: ./.github/actions/setup-k3s
        with:
          kubeconfig_path: /etc/rancher/k3s/k3s.yaml
          wait_kube_system: true
          timeout_api_server_ready_seconds: 300
          timeout_node_ready_seconds: 300
          timeout_kube_system_each_seconds: 10
          max_retries_kube_system_ready: 10

      - name: Get Default objects in kube-system
        run: | 
          kubectl get all -n kube-system

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Install awscli
        uses: ./.github/actions/setup-awscli

  helm-chart-deployment:
    needs: [setup-runner, k3s-deployment]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 15
    strategy:
      matrix:
        output-mode: [aggregate, trace, resctrl]
    env:
      RELEASE_NAME: collector-${{ matrix.output-mode }}
      S3_BUCKET: "unvariance-collector-test-irsa"  # Same bucket used in IAM role testing
      AWS_REGION: ${{ secrets.AWS_REGION }}
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      HOME: /root
      # Use provided image repos/tags (defaults are local/*)
      IMAGE_REPOSITORY: ${{ inputs.collector-image-repo }}
      IMAGE_TAG: ${{ inputs.collector-image-tag || 'latest' }}
      NRI_INIT_REPOSITORY: ${{ inputs.nri-init-image-repo }}
      NRI_INIT_TAG: ${{ inputs.nri-init-tag || 'latest' }}
      OUTPUT_MODE: ${{ matrix.output-mode }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download collector image artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.collector-image-artifact }}
          path: collector-image

      - name: Download nri-init image artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.nri-init-image-artifact }}
          path: nri-init-image

      - name: Import images into k3s containerd
        run: |
          set -euxo pipefail
          # Import docker image tars into k3s' containerd
          k3s ctr images import collector-image/image.tar
          k3s ctr images import nri-init-image/image.tar
          # Show images
          k3s ctr -n k8s.io images ls | sed -n '1,200p'

      - name: Generate UUID Prefix
        id: generate-uuid
        run: |
          UUID=$(python3 -c "import uuid; print(uuid.uuid4())")
          echo "Using UUID prefix: $UUID"
          echo "uuid=$UUID" >> $GITHUB_OUTPUT

      - name: Raise perf_event mlock limit (trace only)
        if: ${{ matrix.output-mode == 'trace' }}
        run: |
          set -euxo pipefail
          echo "Before: kernel.perf_event_mlock_kb=$(sysctl -n kernel.perf_event_mlock_kb)"
          # Increase limit to 1 GiB to accommodate large per-CPU perf ring buffers
          (sysctl -w kernel.perf_event_mlock_kb=1048576) || (sudo sysctl -w kernel.perf_event_mlock_kb=1048576)
          # Loosen paranoid if needed (non-fatal if already permissive)
          (sysctl -w kernel.perf_event_paranoid=2 || true) || (sudo sysctl -w kernel.perf_event_paranoid=2 || true)
          echo "After:  kernel.perf_event_mlock_kb=$(sysctl -n kernel.perf_event_mlock_kb)"

      - name: Deploy Collector Helm Chart
        run: |
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}-"

          # Derive trace flag from output mode
          case "${OUTPUT_MODE}" in
            trace)
              TRACE_FLAG=true
              ;;
            *)
              TRACE_FLAG=false
              ;;
          esac

          # Create values override file common to all modes
          cat > values-override.yaml << EOF
          image:
            repository: "${IMAGE_REPOSITORY}"
            tag: "${IMAGE_TAG}"

          collector:
            verbose: true
            trace: ${TRACE_FLAG}

          storage:
            type: "s3"
            prefix: "${UUID_PREFIX}"
            s3:
              bucket: "${S3_BUCKET}"
              region: "${AWS_REGION}"
              auth:
                method: "iam"  # Using IAM role

          nri:
            init:
              image:
                repository: "${NRI_INIT_REPOSITORY}"
                tag: "${NRI_INIT_TAG}"
          EOF

          # For trace mode, increase memory limits to 3x chart defaults
          if [ "${OUTPUT_MODE}" = "trace" ]; then
            # Chart defaults: limits.memory=350Mi, requests.memory=250Mi
            # Use 3x to avoid OOM during trace runs: 1050Mi / 750Mi
            cat >> values-override.yaml << EOF
          resources:
            limits:
              memory: 1050Mi
            requests:
              memory: 750Mi
          EOF
          fi

          # For resctrl mode, enable resctrl and ensure outputs share the UUID prefix
          if [ "${OUTPUT_MODE}" = "resctrl" ]; then
            cat >> values-override.yaml << EOF
          resctrl:
            enabled: true
            autoMountHost: true
            prefix: "resctrl-occupancy-${UUID_PREFIX}"
          EOF
          fi

          # Print the values being used
          echo "Using image: ${IMAGE_REPOSITORY}:${IMAGE_TAG}"
          echo "Output mode: ${OUTPUT_MODE} (trace=${TRACE_FLAG})"

          # Install/upgrade the helm chart
          helm upgrade --install ${RELEASE_NAME} ./charts/collector -f values-override.yaml

      - name: Wait for Collector Pods to be Ready
        run: |
          # Allow the wait to fail so we can dump diagnostics
          set +e
          kubectl wait --for=condition=Ready pods --timeout=60s -l app.kubernetes.io/name=collector
          STATUS=$?
          set -e
          if [ "$STATUS" -ne 0 ]; then
            echo "Collector pods are not ready after timeout. Dumping diagnostics..."
            echo "\n==> kubectl get pods (all)"
            kubectl get pods -o wide || true
            echo "\n==> kubectl describe pods (collector-labeled)"
            kubectl describe pods -l app.kubernetes.io/name=collector || true
            echo "\n==> kubectl get events (sorted)"
            kubectl get events --sort-by=.lastTimestamp || true
            echo "\n==> kubectl logs for collector pods (all containers)"
            for p in $(kubectl get pods -l app.kubernetes.io/name=collector -o jsonpath='{.items[*].metadata.name}'); do
              echo "\n---- Logs for pod: $p ----"
              kubectl logs "$p" --all-containers=true --tail=-1 || true
            done
            exit 1
          fi

      - name: Show Pod Status
        run: |
          kubectl get pods
          kubectl describe pods -l app.kubernetes.io/name=collector

      - name: Display logs while collector runs for a while
        run: |
          timeout 10s kubectl logs -f -l app.kubernetes.io/name=collector || true
      
      - name: Uninstall Collector Helm Chart
        run: |
          helm uninstall ${RELEASE_NAME} --wait --timeout=60s

      - name: Collector logs
        run: |
          kubectl logs -l app.kubernetes.io/name=collector || true
          
      - name: Check for Files in S3
        run: |
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}"
          if [ "${OUTPUT_MODE}" = "resctrl" ]; then
            SEARCH_PREFIX="resctrl-occupancy-${UUID_PREFIX}"
          else
            SEARCH_PREFIX="${UUID_PREFIX}"
          fi
          echo "Checking for files with prefix ${SEARCH_PREFIX} in S3 bucket ${S3_BUCKET}"
          
          # List files with the UUID prefix
          S3_FILES=$(aws s3 ls "s3://${S3_BUCKET}/${SEARCH_PREFIX}" --recursive || echo "")
          
          if [ -z "$S3_FILES" ]; then
            echo "No files found with prefix ${SEARCH_PREFIX} in bucket ${S3_BUCKET}"
            exit 1
          else
            echo "Found files with prefix ${SEARCH_PREFIX}:"
            echo "$S3_FILES"
            
            # Get the first file path
            FIRST_FILE=$(echo "$S3_FILES" | head -n 1 | awk '{print $4}')
            
            # Download the file for validation
            aws s3 cp "s3://${S3_BUCKET}/${FIRST_FILE}" /tmp/test-parquet.parquet
            
            # Check file size
            FILE_SIZE=$(stat -c %s /tmp/test-parquet.parquet)
            echo "Downloaded file size: ${FILE_SIZE} bytes"
            
            # We could add parquet validation here if a parquet tool is available
            echo "Helm chart S3 integration test successful"
          fi
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        with:
          name: helm-chart-test-results-${{ matrix.output-mode }}
          path: /tmp/test-parquet.parquet
          if-no-files-found: warn

  verify-artifacts:
    name: Verify Parquet Artifacts
    needs: [helm-chart-deployment]
    runs-on: ubuntu-latest
    if: always()  # Run even if helm-chart-deployment fails
    strategy:
      matrix:
        output-mode: [aggregate, trace, resctrl]
    steps:
      - name: Download Artifacts
        uses: actions/download-artifact@v4
        with:
          name: helm-chart-test-results-${{ matrix.output-mode }}
          path: parquet-data

      - name: Install pqrs
        run: |
          curl -L -o pqrs.zip https://github.com/manojkarthick/pqrs/releases/download/v0.3.2/pqrs-0.3.2-x86_64-unknown-linux-gnu.zip
          python3 -m zipfile -e pqrs.zip .
          sudo mv pqrs-0.3.2-x86_64-unknown-linux-gnu/bin/pqrs /usr/local/bin/
          sudo chmod +x /usr/local/bin/pqrs
          rm -rf pqrs.zip pqrs-0.3.2-x86_64-unknown-linux-gnu
          pqrs --version

      - name: Verify Parquet File Schema and Contents
        run: |
          echo "Verifying Parquet files for ${{ matrix.output-mode }} mode..."
          
          # Find the parquet file
          PARQUET_FILE=$(find parquet-data -name "*.parquet" -type f | head -n 1)
          
          if [ -z "$PARQUET_FILE" ]; then
            echo "ERROR: No parquet file found in artifacts"
            exit 1
          fi
          
          echo "Found parquet file: $PARQUET_FILE"
          
          # Check file size
          FILE_SIZE=$(stat -c %s "$PARQUET_FILE")
          echo "File size: ${FILE_SIZE} bytes"
          
          if [ "$FILE_SIZE" -eq 0 ]; then
            echo "ERROR: Parquet file is empty"
            exit 1
          fi
          
          # Generate and examine schema
          echo "Generating schema..."
          pqrs schema "$PARQUET_FILE" > schema.txt
          cat schema.txt
          
          # Define field lists for different output modes
          case "${{ matrix.output-mode }}" in
            trace)
              echo "Setting up trace mode field verification..."
              REQUIRED_FIELDS=("pid" "timestamp" "cpu_id" "is_context_switch" "cgroup_id" "cache_references" "cycles" "instructions" "llc_misses" "next_tgid" "pod_name" "pod_namespace" "pod_uid" "container_name" "container_id")
              ;;
            aggregate)
              echo "Setting up aggregate mode field verification..."
              REQUIRED_FIELDS=("pid" "start_time" "cgroup_id" "cache_references" "cycles" "instructions" "llc_misses" "pod_name" "pod_namespace" "pod_uid" "container_name" "container_id")
              ;;
            resctrl)
              echo "Setting up resctrl mode field verification..."
              REQUIRED_FIELDS=("start_timestamp" "timestamp" "pod_namespace" "pod_name" "pod_uid" "resctrl_group" "llc_occupancy_bytes")
              ;;
          esac
          
          # Verify all required fields are present in schema (including NRI enrichment)
          echo "Verifying required fields in schema..."
          for field in "${REQUIRED_FIELDS[@]}"; do
            if ! grep -q "$field" schema.txt; then
              echo "ERROR: Required field '$field' not found in schema"
              exit 1
            else
              echo "✓ Found required field: $field"
            fi
          done
          
          # For aggregate/trace, perform a light diversity check; skip for resctrl
          if [ "${{ matrix.output-mode }}" != "resctrl" ]; then
            echo "Sampling records with JSON output..."
            pqrs sample --records 100 --json "$PARQUET_FILE" > sample.json

            echo "Verifying field values have sufficient diversity..."
            # Fields that should only warn (VM might not support these performance counters)
            WARN_ONLY_FIELDS=("cache_references" "llc_misses")

            for field in "${REQUIRED_FIELDS[@]}"; do
              echo "Checking field: $field"
              UNIQUE_VALUES=$(jq -r ".${field} // null" sample.json | sort -u | wc -l)
              echo "  Unique values in $field: $UNIQUE_VALUES"
              if [ "$UNIQUE_VALUES" -lt 2 ]; then
                if [[ " ${WARN_ONLY_FIELDS[*]} " =~ " ${field} " ]]; then
                  echo "  WARNING: Field '$field' has <2 different values ($UNIQUE_VALUES)"
                else
                  echo "  ERROR: Field '$field' has <2 different values ($UNIQUE_VALUES)"
                  exit 1
                fi
              else
                echo "  ✓ Field '$field' has sufficient diversity"
              fi
            done
          else
            echo "Skipping diversity checks for resctrl mode"
          fi

          echo "Parquet file verification completed successfully for ${{ matrix.output-mode }} mode"


  stop-runner:
    name: Stop EC2 runner
    needs: [setup-runner, k3s-deployment, helm-chart-deployment]
    runs-on: ubuntu-latest
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Stop AWS Runner
        uses: ./.github/actions/aws-runner/cleanup
        with:
          runner-label: ${{ needs.setup-runner.outputs.runner-label }}
          ec2-instance-id: ${{ needs.setup-runner.outputs.ec2-instance-id }}
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ needs.setup-runner.outputs.region }}
