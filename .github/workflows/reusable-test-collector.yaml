name: reusable-test-collector

on:
  workflow_call:
    inputs:
      level:
        description: Test level (cheap-short|perf-short|full-short|full-long)
        required: true
        type: string
      collector-binary-artifact:
        description: Name of the collector binary artifact to consume
        required: false
        default: collector-binary
        type: string
      instance-type:
        description: EC2 instance type for heavy tests
        required: false
        default: m7i.metal-24xl
        type: string
      run-multi-kernel-tests:
        description: Whether to run multi-kernel tests (requires QEMU/LVH capable runner)
        required: false
        default: false
        type: boolean
    secrets:
      AWS_ROLE_ARN:
        required: false
      REPO_ADMIN_TOKEN:
        required: false
      AWS_REGION:
        required: false
      S3_ACCESS_KEY_ID:
        required: false
      S3_SECRET_ACCESS_KEY:
        required: false

jobs:
  bpf-cgroup-inode-test:
    name: BPF cgroup inode assumptions (GH-hosted)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Install dependencies
        uses: awalsh128/cache-apt-pkgs-action@v1
        with:
          packages: clang libelf-dev unzip
          version: 1.0

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Cargo cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

      - name: Test cgroup inode assumptions
        run: |
          cargo test --package bpf --test cgroup_inode_test --no-run --verbose
          
          echo "Testing that the cgroup ID from BPF matches the inode number in the filesystem"
          echo "This is critical for container identification in the collector"
          
          TEST_BIN=$(find target/debug -name "cgroup_inode_test-*" -type f -executable | head -1)
          if [ -z "$TEST_BIN" ]; then echo "Missing test bin"; exit 1; fi
          # Run ignored tests since this one requires BPF privileges
          sudo "$TEST_BIN" --ignored

  test-multi-kernel:
    needs: [setup-runner, prepare-runner]
    if: inputs.run-multi-kernel-tests == true
    uses: ./.github/workflows/reusable-test-multi-kernel-collector.yaml
    with:
      runner-label: ${{ needs.setup-runner.outputs.runner-label }}
      collector-binary-artifact: ${{ inputs.collector-binary-artifact }}
      
  setup-runner:
    if: inputs.level != 'cheap-short'
    name: Start EC2 runner
    runs-on: ubuntu-latest
    outputs:
      runner-label: ${{ steps.start-runner.outputs.runner-label }}
      ec2-instance-id: ${{ steps.start-runner.outputs.ec2-instance-id }}
      region: ${{ steps.start-runner.outputs.region }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Start AWS Runner
        id: start-runner
        uses: ./.github/actions/aws-runner
        with:
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          iam-role-name: github-actions-runner
          instance-type: ${{ inputs.instance-type }}
          image-type: ubuntu-24.04
          volume-size: '40'

  cancel-on-failure:
    needs: setup-runner
    runs-on: ubuntu-latest
    if: failure()
    steps:
      - name: Cancel workflow
        uses: andymckay/cancel-action@a955d435292c0d409d104b57d8e78435a93a6ef1

  test-ebpf:
    if: inputs.level != 'cheap-short'
    name: Collector local run (parquet verification)
    needs: [setup-runner, prepare-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 10
    steps:
      - name: Download collector binary
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.collector-binary-artifact }}
          path: ./

      - name: Make collector executable
        run: chmod +x ./collector

      - name: Run eBPF collector
        run: |
          # Run with sudo since eBPF programs require elevated privileges
          sudo ./collector -d 10 --storage-type local --prefix "/tmp/metrics-"

      - name: Verify parquet output
        run: |
          # Get the parquet file name based on the prefix /tmp/metrics
          parquet_file=$(find /tmp -name "metrics-*.parquet")

          # Print parquet file contents as CSV
          echo "Parquet file contents:"
          pqrs cat --csv $parquet_file

      - name: Upload parquet file
        uses: actions/upload-artifact@v4
        with:
          name: metrics-parquet
          path: /tmp/metrics-*.parquet
          if-no-files-found: error

  prepare-runner:
    needs: [setup-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 5
    steps:
      - name: Checkout .github folder
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          sparse-checkout: |
            .github
          sparse-checkout-cone: true

      - name: Install awscli
        uses: ./.github/actions/setup-awscli

      - name: Install pqrs
        run: |
          curl -L -o pqrs.zip https://github.com/manojkarthick/pqrs/releases/download/v0.3.2/pqrs-0.3.2-x86_64-unknown-linux-gnu.zip
          python3 -m zipfile -e pqrs.zip .
          sudo mv pqrs-0.3.2-x86_64-unknown-linux-gnu/bin/pqrs /usr/local/bin/
          sudo chmod +x /usr/local/bin/pqrs
          rm -rf pqrs.zip pqrs-0.3.2-x86_64-unknown-linux-gnu

      - name: Install Podman packages
        uses: awalsh128/cache-apt-pkgs-action@v1
        with:
          packages: podman podman-docker
          version: 1.0

      - name: Install Podman and Docker compatibility
        run: |
          # Verify installation
          podman --version
          docker --version
          
          # Start podman socket for Docker API compatibility
          sudo systemctl enable --now podman.socket
          sudo systemctl status podman.socket
      

  test-ebpf-s3:
    if: inputs.level != 'cheap-short'
    name: Collector run + S3 validation
    needs: [setup-runner, prepare-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 15
    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      IRSA_BUCKET: "unvariance-collector-test-irsa"
      KEY_AUTH_BUCKET: "unvariance-collector-test-key-auth"
      AWSCLI: "/usr/local/bin/aws"
    steps:
      - name: Download collector binary
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.collector-binary-artifact }}
          path: ./

      - name: Make collector executable
        run: chmod +x ./collector

      # Test IAM role-based authentication (IRSA)
      - name: Test S3 with IAM Role Authentication
        id: test-iam-role
        run: |
          # Generate a unique prefix for this test
          IRSA_PREFIX=$(python3 -c "import uuid; print(uuid.uuid4())")
          echo "Using IRSA prefix: $IRSA_PREFIX"
          echo "irsa_prefix=$IRSA_PREFIX" >> $GITHUB_OUTPUT
          
          # Run collector with S3 output using IAM role
          echo "Running collector with IAM role authentication..."
          sudo -E AWS_BUCKET_NAME=${IRSA_BUCKET} RUST_LOG=debug ./collector -d 10 --storage-type s3 --prefix "${IRSA_PREFIX}/"
          
          # Verify the upload succeeded
          echo "Verifying S3 upload with IAM role..."
          $AWSCLI s3 ls "s3://${IRSA_BUCKET}/${IRSA_PREFIX}/"
          
          # Get uploaded file(s)
          IRSA_FILES=$($AWSCLI s3 ls "s3://${IRSA_BUCKET}/${IRSA_PREFIX}/" --recursive | awk '{print $4}')
          if [ -z "$IRSA_FILES" ]; then
            echo "No files found in IRSA bucket with prefix ${IRSA_PREFIX}"
            exit 1
          fi
          
          # Download and validate first file
          FIRST_FILE=$(echo "$IRSA_FILES" | head -n 1)
          echo "Downloading and validating file: ${FIRST_FILE}"
          $AWSCLI s3 cp "s3://${IRSA_BUCKET}/${FIRST_FILE}" /tmp/irsa-test.parquet
          
          # Validate parquet file
          echo "Validating parquet file structure:"
          pqrs cat --csv /tmp/irsa-test.parquet
          echo "IRSA test successful"

      # Test Access Key-based authentication
      - name: Test S3 with Access Key Authentication
        id: test-access-key
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
        run: |
          # Generate a unique prefix for this test
          KEY_PREFIX=$(python3 -c "import uuid; print(uuid.uuid4())")
          echo "Using Key Auth prefix: $KEY_PREFIX"
          echo "key_prefix=$KEY_PREFIX" >> $GITHUB_OUTPUT
          
          # Run collector with S3 output using access keys
          echo "Running collector with Access Key authentication..."
          sudo -E AWS_BUCKET_NAME=${KEY_AUTH_BUCKET} RUST_LOG=debug ./collector -d 10 --storage-type s3 --prefix "${KEY_PREFIX}/"
          
          # Verify the upload succeeded
          echo "Verifying S3 upload with Access Key..."
          $AWSCLI s3 ls "s3://${KEY_AUTH_BUCKET}/${KEY_PREFIX}/"
          
          # Get uploaded file(s)
          KEY_FILES=$($AWSCLI s3 ls "s3://${KEY_AUTH_BUCKET}/${KEY_PREFIX}/" --recursive | awk '{print $4}')
          if [ -z "$KEY_FILES" ]; then
            echo "No files found in Key Auth bucket with prefix ${KEY_PREFIX}"
            exit 1
          fi
          
          # Download and validate first file
          FIRST_FILE=$(echo "$KEY_FILES" | head -n 1)
          echo "Downloading and validating file: ${FIRST_FILE}"
          $AWSCLI s3 cp "s3://${KEY_AUTH_BUCKET}/${FIRST_FILE}" /tmp/key-auth-test.parquet
          
          # Validate parquet file
          echo "Validating parquet file structure:"
          pqrs cat --csv /tmp/key-auth-test.parquet
          echo "Access Key test successful"

      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: s3-test-parquet-files
          path: |
            /tmp/irsa-test.parquet
            /tmp/key-auth-test.parquet
          if-no-files-found: error

  stop-runner:
    name: Stop EC2 runner
    needs: [setup-runner, test-ebpf, test-ebpf-s3, test-multi-kernel, nri-enrichment-e2e]
    runs-on: ubuntu-latest
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Stop AWS Runner
        uses: ./.github/actions/aws-runner/cleanup
        with:
          runner-label: ${{ needs.setup-runner.outputs.runner-label }}
          ec2-instance-id: ${{ needs.setup-runner.outputs.ec2-instance-id }}
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ needs.setup-runner.outputs.region }}


  nri-enrichment-e2e:
    if: inputs.level == 'full-short' || inputs.level == 'full-long'
    name: NRI enrichment E2E (k3s)
    needs: [setup-runner, prepare-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 20
    env:
      POD_NAME: nri-enrichment-test
      POD_NAMESPACE: default
      OUTPUT_PREFIX: /tmp/nri-e2e-
      NRI_SOCKET_PATH: /var/run/nri/nri.sock
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download collector binary
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.collector-binary-artifact }}
          path: ./

      - name: Make collector executable
        run: chmod +x ./collector

      - name: Setup k3s cluster
        uses: ./.github/actions/setup-k3s
        with:
          kubeconfig_path: /etc/rancher/k3s/k3s.yaml
          preflight_inotify: true
          disable_packaged_addons: true
          timeout_api_server_ready_seconds: 300
          timeout_node_ready_seconds: 300

      - name: Ensure NRI socket exists
        uses: ./.github/actions/nri-socket-exists

      - name: Install jq
        uses: awalsh128/cache-apt-pkgs-action@v1
        with:
          packages: jq
          version: 1.0

      - name: Deploy test pod (${{ env.POD_NAME }})
        run: |
          cat > pod.yaml << EOF
          apiVersion: v1
          kind: Pod
          metadata:
            name: ${POD_NAME}
            namespace: ${POD_NAMESPACE}
            labels:
              app: nri-enrichment-test
          spec:
            restartPolicy: Never
            containers:
              - name: tester
                image: busybox:1.36
                command: ["sh", "-c", "yes > /dev/null & yes > /dev/null & sleep 120"]
          EOF
          kubectl apply -f pod.yaml
          kubectl wait --for=condition=Ready pod/${POD_NAME} -n ${POD_NAMESPACE} --timeout=120s
          kubectl get pod ${POD_NAME} -n ${POD_NAMESPACE} -o wide

      - name: Run collector (binary on host)
        env:
          NRI_SOCKET_PATH: ${{ env.NRI_SOCKET_PATH }}
          RUST_LOG: debug
        run: |
          echo "Running collector for 25 seconds..."
          sudo -E ./collector -d 25 --storage-type local --prefix "${OUTPUT_PREFIX}" --verbose

      - name: Verify enrichment fields and values
        run: |
          PARQUET=$(ls -1 ${OUTPUT_PREFIX}*.parquet | head -n1)
          if [ -z "$PARQUET" ]; then
            echo "No parquet output found with prefix ${OUTPUT_PREFIX}"
            exit 1
          fi
          echo "Using parquet file: $PARQUET"
          echo "Schema:"
          pqrs schema "$PARQUET"
          # Verify fields exist
          for f in pod_name pod_namespace pod_uid container_name container_id process_name cgroup_id; do
            if ! pqrs schema "$PARQUET" | grep -q "$f"; then
              echo "Missing expected field: $f"
              exit 1
            fi
          done
          # Filter rows for our pod and check CPU-ish process names
          pqrs cat --json "$PARQUET" | jq -c "select(.pod_name==\"$POD_NAME\")" > /tmp/nri-rows.json || true
          COUNT=$(wc -l < /tmp/nri-rows.json | tr -d ' ')
          echo "Rows for pod $POD_NAME: $COUNT"
          if [ "$COUNT" -lt 1 ]; then
            echo "No rows found for pod $POD_NAME; enrichment likely failed"
            echo "Sample of all rows for debugging:"
            pqrs sample --json --records 50 "$PARQUET" || true
            exit 1
          fi
          # Verify container name matches
          CNAMES=$(jq -r .container_name /tmp/nri-rows.json | sort -u | tr '\n' ' ')
          echo "Container names: $CNAMES"
          if ! echo "$CNAMES" | grep -qw "tester"; then
            echo "Expected container_name 'tester' not found"
            exit 1
          fi
          # Verify process name includes expected comm (yes)
          COMMS=$(jq -r .process_name /tmp/nri-rows.json | sort -u | tr '\n' ' ')
          echo "Process names: $COMMS"
          if ! echo "$COMMS" | grep -qw "$EXPECTED_COMM"; then
            echo "Expected process_name '$EXPECTED_COMM' not found among: $COMMS"
            exit 1
          fi
          # Verify namespace
          NSES=$(jq -r .pod_namespace /tmp/nri-rows.json | sort -u | tr '\n' ' ')
          echo "Namespaces: $NSES"
          if ! echo "$NSES" | grep -qw "$POD_NAMESPACE"; then
            echo "Expected pod_namespace '$POD_NAMESPACE' not found"
            exit 1
          fi
          echo "✅ NRI enrichment verified for pod $POD_NAME"

      - name: Upload E2E parquet
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nri-e2e-parquet
          path: ${{ env.OUTPUT_PREFIX }}*.parquet
          if-no-files-found: warn
